{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephaan/intellect-supreme/blob/master/WxGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2zn9z_RlMCL",
        "outputId": "036bc5d0-6035-42aa-b326-5a8cf48cfed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95fs8K_slEyR",
        "outputId": "c132de67-c6e6-4c2f-afdc-b76e0caaf617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c27e8f6d930>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install pathlib\n",
        "!pip install torchinfo\n",
        "!pip install tiktoken\n",
        "import torch.nn.utils as utils\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import math\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac16d6ef"
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "files_no = 1000 # number of input text files to read\n",
        "train_test_split = 0.85\n",
        "block_size = 624 # number of tokens in one block. Maximum context sequence.\n",
        "batch_size = 16 # size of the batch with blocks\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "51aa8149f07a46bf9769d13f132116d6",
            "719b3cdde4a5433da6231f69c86a0059",
            "f5576abaa063471e9550dc050814cb3f",
            "23035394b9c24fe289d749579d99babb",
            "cb877dfc72684d148743e4ddaa0e6c29",
            "ed95e740172f4b50ae108727c699b234",
            "d053906d458841fe990d193f1d75c2e2",
            "dd778e8822db4b6282855caa85fddb31",
            "c7ad2a771fae4323a7348f59fb661279",
            "5e2e744bb66f4fb3871a04b89eeefc1c",
            "41d3b027f42e4a33bccdbd9a5231e591"
          ]
        },
        "id": "GpQ8BU5tPlXF",
        "outputId": "5c93020e-e955-47fe-93da-bd77b807bb36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51aa8149f07a46bf9769d13f132116d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read and merge all data files\n",
        "data_dir = Path(\"/content/drive/MyDrive/adam_s/data\")\n",
        "text_paths = list(data_dir.glob(\"*\"))\n",
        "\n",
        "text = ''\n",
        "\n",
        "for file in tqdm(text_paths[:files_no]):\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        inp = f.read()\n",
        "        text += inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9c2adbe",
        "outputId": "4efe1470-e6b6-428b-bf83-9b16b334af3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length:  440362672\n",
            "Dirichlet Allocation (LDA)\n",
            "\tHidden Markov models\n",
            "\tDeep Boltzmann machines\n",
            "\tVAEs\n",
            "\tGANs\n",
            "\tNaive Bayes classifiers, though named as a discriminative model, utilize Bayes' theorem to learn the joint distribution of X and Y under the assumption that the X variables are independent. Similarly, Gaussian mixture models describe the likelihood of a data point belonging to one of a group of normal distributions using the joint probability of the label and these distributions.\n",
            "\tLDA represents a document as the joint probability of a word and a set of underlying keyword lists (topics) that are used in a document. Hidden Markov models express the joint probability of a state and the next state of data, such as the weather on successive days of the week. As you will see in Chapter 4, Teaching Networks to Generate Digits, deep Boltzmann machines learn the joint probability of a label and the data vector it is associated with. The VAE and GAN models we will cover in Chapters 5, 6, 7, and 11 also utilize joint distributions to map between complex data types. This mapping allows us to generate data from random vectors or transform one kind of data into another.\n",
            "\tAs already mentioned, another view of generative models is that they allow us to generate samples of X if we know an outcome, Y. In the first four models in the previous list, this conditional probability is just a component of the model formula, with the posterior estimates still being the ultimate objective. However, in the last three examples, which are all deep neural network models, learning the conditional of X dependent upon a hidden, or latent, variable, Z, is actually the main objective, in order to generate new data samples. Using the rich structure allowed by multi-layered neural networks, these models can approximate the distribution of complex data types such as images, natural language, and sound. Also, instead of being a target value, Z is often a random number in these applications, serving merely as an input from which to generate a large space of hypothetical data points. To the extent we have a label (such as whether a generated image should be of a dog or dolphin, or the genre of a generated song), the model is P(X|Y=y, Z=z), where the label Y controls the generation of data that is otherwise unrestricted by the random nature of Z.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Why use generative models?\n",
            "\n",
            "\n",
            "\tNow that we have reviewed what generative models are and defined them more formally in the language of probability, why would we have a need for such models in the first place? What value do they provide in practical applications? To answer this question, let's take a brief tour of the topics that we will cover in more detail in the rest of this book.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The promise of deep learning\n",
            "\n",
            "\n",
            "\tAs noted already, many of the models we will survey in the book are deep, multi-level neural networks. The last 15 years have seen a renaissance in the development of deep learning models for image classification, natural language processing and understanding, and reinforcement learning. These advances were enabled by breakthroughs in traditional challenges in tuning and optimizing very complex models, combined with access to larger datasets, distributed computational power in the cloud, and frameworks such as TensorFlow that make it easier to prototype and reproduce research.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building a better digit classifier\n",
            "\n",
            "\n",
            "\tA classic problem used to benchmark algorithms in machine learning and computer vision is the task of classifying which handwritten digit from 0-9 is represented in a pixelated image from the MNIST dataset.17 A large breakthrough on this problem occurred in 2006, when researchers at the University of Toronto and the National University of Singapore discovered a way to train deep neural networks to perform this task.18\n",
            "\tOne of their critical observations was that instead of training a network to directly predict the most likely digit (Y) given an image (X), it was more effective to first train a network that could generate images, and then classify them as a second step. In Chapter 4, Teaching Networks to Generate Digits, I will describe how this model improved upon past attempts, and how to create your own restricted Boltzmann machine and deep Boltzmann machine models that can generate new MNIST digit images.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating images\n",
            "\n",
            "\n",
            "\tA challenge to generating images such as the Portrait of Edmond Belamy with the approach used for the MNIST dataset is that frequently, images have no labels (such as a digit); rather, we want to map the space of random numbers into a set of artificial images using a latent vector, Z, as I described earlier in the chapter.\n",
            "\tA further constraint is that we want to promote diversity of these images. If we input numbers within a certain range, we would like to know that they generate different outputs, and be able to tune the resulting image features. For this purpose, VAEs were developed to generate diverse and photorealistic images (Figure 1.5).\n",
            "\tFigure 1.5: \n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset length: \", len(text))\n",
        "print(text[30000:35000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqL_rgpQlEyb",
        "outputId": "ecea19f0-46fa-4b3c-ec90-4c051ba57790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "954\n",
            "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x93', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'ă', 'ą', 'Ć', 'ć', 'ĉ', 'Č', 'č', 'đ', 'Ē', 'ē', 'ę', 'ě', 'Ĝ', 'ğ', 'Ġ', 'ģ', 'ħ', 'Ī', 'ī', 'ı', 'ĵ', 'ĺ', 'Ł', 'ł', 'ń', 'ŋ', 'Ō', 'ō', 'ő', 'Œ', 'œ', 'ř', 'Ś', 'ś', 'Ş', 'ş', 'Š', 'š', 'ţ', 'ũ', 'Ū', 'ū', 'ŭ', 'ů', 'ű', 'ŵ', 'Ŷ', 'ŷ', 'Ÿ', 'ź', 'ż', 'Ž', 'ž', 'Ǝ', 'Ɛ', 'ƒ', 'Ǎ', 'ǎ', 'Ǐ', 'ǐ', 'Ǒ', 'ǒ', 'ǔ', 'Ǣ', 'ǣ', 'ǧ', 'ǫ', 'Ǭ', 'ǽ', 'ǿ', 'Ȝ', 'ȝ', 'ȣ', 'ȧ', 'ɑ', 'ə', 'ɛ', 'ɪ', 'ɫ', 'ɴ', 'ʜ', 'ʹ', 'ʺ', 'ʻ', 'ʼ', 'ʾ', 'ʿ', '˃', 'ˆ', 'ˇ', '˗', '˙', '˚', '˜', '˝', '́', '̂', '̃', '̄', '̇', '̈', '̓', '̔', '̣', '̥', '΄', '·', 'Έ', 'Ί', 'Ό', 'ΐ', 'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ξ', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω', 'ά', 'έ', 'ή', 'ί', 'ΰ', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ϊ', 'ϋ', 'ό', 'ύ', 'ώ', 'ϑ', 'ϕ', 'ϖ', 'Ϛ', 'ϛ', 'Ϡ', 'ϭ', 'ϰ', 'ϵ', 'А', 'В', 'Г', 'Е', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Ц', 'Ш', 'Ь', 'Я', 'а', 'б', 'в', 'д', 'е', 'з', 'и', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ч', 'я', 'є', 'ѡ', 'ӓ', 'ә', 'ӱ', '֗', '׀', 'א', 'ב', 'ו', 'י', 'כ', 'ל', 'ם', 'ש', 'ת', 'ر', 'ص', 'ف', 'ज', 'ब', 'ल', 'ी', 'े', 'ᑠ', 'ᴀ', 'ᴅ', 'ᴍ', 'ᴏ', 'ᵉ', 'ᵐ', 'ᵗ', 'Ḃ', 'ḍ', 'Ḟ', 'Ḡ', 'ḡ', 'Ḥ', 'ḥ', 'Ḳ', 'ḳ', 'ṁ', 'Ṃ', 'ṃ', 'ṅ', 'Ṇ', 'ṇ', 'ṗ', 'ṙ', 'Ṛ', 'ṛ', 'Ṡ', 'Ṣ', 'ṣ', 'Ṭ', 'ṭ', 'Ṽ', 'ẁ', 'ẋ', 'ẍ', 'ẏ', 'ẓ', 'ẽ', 'ề', 'ἀ', 'ἁ', 'ἂ', 'ἃ', 'ἄ', 'ἅ', 'ἆ', 'Ἀ', 'Ἁ', 'Ἂ', 'Ἃ', 'Ἄ', 'Ἅ', 'Ἆ', 'ἐ', 'ἑ', 'ἒ', 'ἓ', 'ἔ', 'ἕ', 'Ἐ', 'Ἑ', 'Ἒ', 'Ἓ', 'Ἔ', 'Ἕ', 'ἠ', 'ἡ', 'ἢ', 'ἣ', 'ἤ', 'ἥ', 'ἦ', 'ἧ', 'Ἠ', 'Ἡ', 'Ἢ', 'Ἣ', 'Ἤ', 'Ἥ', 'Ἦ', 'Ἧ', 'ἰ', 'ἱ', 'ἲ', 'ἳ', 'ἴ', 'ἵ', 'ἶ', 'ἷ', 'Ἰ', 'Ἱ', 'Ἴ', 'Ἵ', 'Ἶ', 'ὀ', 'ὁ', 'ὂ', 'ὃ', 'ὄ', 'ὅ', 'Ὀ', 'Ὁ', 'Ὃ', 'Ὄ', 'Ὅ', 'ὐ', 'ὑ', 'ὒ', 'ὓ', 'ὔ', 'ὕ', 'ὖ', 'ὗ', 'Ὑ', 'Ὕ', 'Ὗ', 'ὠ', 'ὡ', 'ὢ', 'ὣ', 'ὤ', 'ὥ', 'ὦ', 'ὧ', 'Ὠ', 'Ὡ', 'Ὤ', 'Ὥ', 'Ὦ', 'Ὧ', 'ὰ', 'ά', 'ὲ', 'έ', 'ὴ', 'ή', 'ὶ', 'ί', 'ὸ', 'ό', 'ὺ', 'ύ', 'ὼ', 'ώ', 'ᾀ', 'ᾄ', 'ᾅ', 'ᾆ', 'ᾐ', 'ᾑ', 'ᾒ', 'ᾔ', 'ᾖ', 'ᾗ', 'ᾠ', 'ᾡ', 'ᾤ', 'ᾧ', 'ᾳ', 'ᾴ', 'ᾶ', 'ᾷ', 'Ᾱ', '᾽', '᾿', 'ῂ', 'ῃ', 'ῄ', 'ῆ', 'ῇ', 'ῒ', 'ΐ', 'ῖ', 'ῗ', 'ῢ', 'ΰ', 'ῤ', 'ῥ', 'ῦ', 'Ῥ', 'ῲ', 'ῳ', 'ῴ', 'ῶ', 'ῷ', 'ῼ', '´', '\\u2002', '\\u2003', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u200b', '\\u200c', '\\u200d', '‐', '‑', '‒', '–', '—', '―', '‖', '‘', '’', '‚', '“', '”', '„', '†', '‡', '•', '…', '\\u202f', '′', '″', '‴', '‹', '›', '‽', '⁂', '⁄', '⁕', '\\u2060', '⁰', 'ⁱ', '⁴', '⁵', '⁶', '⁷', '⁸', '⁹', '€', '⃒', 'ℂ', 'ℏ', 'ℑ', 'ℒ', 'ℓ', 'ℕ', '№', 'ℚ', 'ℛ', 'ℝ', '™', 'ℤ', 'Ω', 'Å', 'ℬ', 'ℰ', 'ℱ', 'ℵ', '⅓', '⅔', '⅕', '⅖', '⅗', '⅘', '⅙', '⅚', '⅛', '⅜', '⅝', '⅞', '←', '↑', '→', '↓', '↔', '↝', '↦', '⇄', '⇆', '⇋', '⇌', '⇐', '⇒', '⇔', '⇝', '∀', '∂', '∃', '∅', '∆', '∇', '∈', '∉', '∊', '∏', '∑', '−', '∕', '∖', '∗', '∘', '∙', '√', '∝', '∞', '∠', '∣', '∤', '∥', '∧', '∨', '∩', '∪', '∫', '∬', '∭', '∮', '∴', '∵', '∼', '≃', '≅', '≈', '≊', '≙', '≠', '≡', '≢', '≤', '≥', '≦', '≧', '≪', '≫', '≰', '≲', '≺', '≼', '⊂', '⊃', '⊄', '⊆', '⊓', '⊔', '⊕', '⊗', '⊙', '⊢', '⊥', '⊨', '⊩', '⊬', '⊭', '⊮', '⊻', '⋀', '⋁', '⋂', '⋃', '⋄', '⋅', '⋆', '⋔', '⋘', '⋮', '⋯', '⌈', '⌉', '⌊', '⌋', '〈', '〉', '⎛', '⎜', '⎝', '⎞', '⎟', '⎠', '⎡', '⎢', '⎣', '⎤', '⎥', '⎦', '─', '│', '└', '├', '═', '▄', '█', '▌', '■', '□', '▪', '▲', '△', '▶', '▸', '►', '◁', '◆', '◊', '○', '●', '◦', '☉', '☐', '☿', '♀', '♁', '♂', '♈', '♋', '♍', '♎', '♑', '♓', '♠', '♢', '♣', '♥', '♦', '♮', '♯', '✇', '✓', '✦', '✳', '✴', '✻', '❶', '❷', '❸', '❹', '❺', '❻', '➊', '➋', '➌', '➍', '➎', '➏', '➐', '➑', '➒', '➓', '➝', '➞', '➢', '⟨', '⟩', '⟶', '⤚', '⦵', '⦾', '⦿', '⩽', '⩾', '⸺', '〈', '〉', '《', '》', 'の', 'キ', 'コ', 'ツ', 'ノ', 'ヵ', '・', '㬚', '人', '兴', '城', '天', '容', '市', '日', '本', '河', '白', '皇', '神', '管', '語', '雫', '高', '\\ue2d3', '\\uf001', '\\uf002', '\\uf031', '\\uf062', '\\uf8ee', '\\uf8ef', '\\uf8f0', '\\uf8f9', '\\uf8fa', '\\uf8fb', '\\uf8ff', '\\ufeff', '（', '）', '�', '𝐀', '𝐑', '𝐯', '𝑛', '𝒜', '𝒥', '𝒫', '𝒮', '𝒱', '𝓁', '𝔸', '𝔻', '𝔽', '𝜖']\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(chars)\n",
        "\n",
        "to_int = { c:i for i,c in enumerate(chars) }\n",
        "to_str = { i:c for i,c in enumerate(chars) }\n",
        "encode = lambda s: [to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([to_str[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNerGtEflEyr"
      },
      "outputs": [],
      "source": [
        "# Train/test data split\n",
        "sep = int(train_test_split*len(data))\n",
        "train_data = data[:sep]\n",
        "test_data = data[sep:]\n",
        "\n",
        "def random_batch(mode):\n",
        "    data = train_data if mode == 'train' else test_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "X, Y = random_batch('train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0iXHpSHlEys"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MA5-teXlEyu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6O9OpqzlEy2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "'''\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" A complex feedforward neural network with multiple layers and non-linearities. \"\"\"\n",
        "    def __init__(self, n_embd, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential()\n",
        "        for _ in range(n_layers):\n",
        "            self.net.add_module(\n",
        "                \"linear\", nn.Linear(n_embd, 8 * n_embd)\n",
        "            )\n",
        "            self.net.add_module(\"relu\", nn.ReLU())\n",
        "            self.net.add_module(\"dropout\", nn.Dropout(dropout))\n",
        "        self.net.add_module(\"output\", nn.Linear(8 * n_embd, n_embd))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#Block is the entire transformer except for the cross-attention/\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, n_layer, dropout):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd, n_layer, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcdaJgDGwwPd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIu97IE0liq0"
      },
      "outputs": [],
      "source": [
        "class WxGPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_layer, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size  # Store vocab_size as an instance variable\n",
        "        self.block_size = block_size  # Store block_size as an instance variable\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, n_layer, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def top_k_sampling(self, logits, top_k):\n",
        "        if top_k > 0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_k\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "            logits[indices_to_remove] = float('-inf')\n",
        "        return logits\n",
        "\n",
        "    def generate(self, text, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        # Encode the input text\n",
        "        input_tokens = encode(text)  # You need to define the 'encode' function\n",
        "        device = self.token_embedding_table.weight.device\n",
        "        input_tokens_tensor = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        # Generate new tokens\n",
        "        generated_tokens = self._generate_tokens(input_tokens_tensor, max_new_tokens, temperature, top_k)\n",
        "        # Decode the generated tokens into text\n",
        "        generated_text = decode(generated_tokens[0].tolist())  # You need to define the 'decode' function\n",
        "        return generated_text\n",
        "\n",
        "    def _generate_tokens(self, input_tokens, max_new_tokens, temperature, top_k):\n",
        "        # Get the context tensor\n",
        "        context = input_tokens.clone()\n",
        "        # Generate new tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context to the last block_size tokens\n",
        "            context_cond = context[:, -self.block_size:]\n",
        "            # Get the predictions\n",
        "            logits, _ = self.forward(context_cond)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :].squeeze()\n",
        "            # Apply temperature\n",
        "            logits /= temperature\n",
        "            # Apply top-k sampling if specified\n",
        "            if top_k is not None:\n",
        "                logits = self.top_k_sampling(logits, top_k)\n",
        "\n",
        "            # Sample the next token\n",
        "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            # Append the next token to the context\n",
        "            #context = torch.cat((context, next_token), dim=1)\n",
        "            context = torch.cat((context, next_token.unsqueeze(1)), dim=1)\n",
        "\n",
        "        return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVn-ctmWxNib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJlCt-jaWmRG",
        "outputId": "e9bc6c0e-41a1-483f-aec4-ae4500d390b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110.196858 M parameters\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "WxGPT                                         --\n",
              "├─Embedding: 1-1                              641,088\n",
              "├─Embedding: 1-2                              419,328\n",
              "├─Sequential: 1-3                             --\n",
              "│    └─Block: 2-1                             --\n",
              "│    │    └─MultiHeadAttention: 3-1           1,807,008\n",
              "│    │    └─FeedFoward: 3-2                   7,231,392\n",
              "│    │    └─LayerNorm: 3-3                    1,344\n",
              "│    │    └─LayerNorm: 3-4                    1,344\n",
              "│    └─Block: 2-2                             --\n",
              "│    │    └─MultiHeadAttention: 3-5           1,807,008\n",
              "│    │    └─FeedFoward: 3-6                   7,231,392\n",
              "│    │    └─LayerNorm: 3-7                    1,344\n",
              "│    │    └─LayerNorm: 3-8                    1,344\n",
              "│    └─Block: 2-3                             --\n",
              "│    │    └─MultiHeadAttention: 3-9           1,807,008\n",
              "│    │    └─FeedFoward: 3-10                  7,231,392\n",
              "│    │    └─LayerNorm: 3-11                   1,344\n",
              "│    │    └─LayerNorm: 3-12                   1,344\n",
              "│    └─Block: 2-4                             --\n",
              "│    │    └─MultiHeadAttention: 3-13          1,807,008\n",
              "│    │    └─FeedFoward: 3-14                  7,231,392\n",
              "│    │    └─LayerNorm: 3-15                   1,344\n",
              "│    │    └─LayerNorm: 3-16                   1,344\n",
              "│    └─Block: 2-5                             --\n",
              "│    │    └─MultiHeadAttention: 3-17          1,807,008\n",
              "│    │    └─FeedFoward: 3-18                  7,231,392\n",
              "│    │    └─LayerNorm: 3-19                   1,344\n",
              "│    │    └─LayerNorm: 3-20                   1,344\n",
              "│    └─Block: 2-6                             --\n",
              "│    │    └─MultiHeadAttention: 3-21          1,807,008\n",
              "│    │    └─FeedFoward: 3-22                  7,231,392\n",
              "│    │    └─LayerNorm: 3-23                   1,344\n",
              "│    │    └─LayerNorm: 3-24                   1,344\n",
              "│    └─Block: 2-7                             --\n",
              "│    │    └─MultiHeadAttention: 3-25          1,807,008\n",
              "│    │    └─FeedFoward: 3-26                  7,231,392\n",
              "│    │    └─LayerNorm: 3-27                   1,344\n",
              "│    │    └─LayerNorm: 3-28                   1,344\n",
              "│    └─Block: 2-8                             --\n",
              "│    │    └─MultiHeadAttention: 3-29          1,807,008\n",
              "│    │    └─FeedFoward: 3-30                  7,231,392\n",
              "│    │    └─LayerNorm: 3-31                   1,344\n",
              "│    │    └─LayerNorm: 3-32                   1,344\n",
              "│    └─Block: 2-9                             --\n",
              "│    │    └─MultiHeadAttention: 3-33          1,807,008\n",
              "│    │    └─FeedFoward: 3-34                  7,231,392\n",
              "│    │    └─LayerNorm: 3-35                   1,344\n",
              "│    │    └─LayerNorm: 3-36                   1,344\n",
              "│    └─Block: 2-10                            --\n",
              "│    │    └─MultiHeadAttention: 3-37          1,807,008\n",
              "│    │    └─FeedFoward: 3-38                  7,231,392\n",
              "│    │    └─LayerNorm: 3-39                   1,344\n",
              "│    │    └─LayerNorm: 3-40                   1,344\n",
              "│    └─Block: 2-11                            --\n",
              "│    │    └─MultiHeadAttention: 3-41          1,807,008\n",
              "│    │    └─FeedFoward: 3-42                  7,231,392\n",
              "│    │    └─LayerNorm: 3-43                   1,344\n",
              "│    │    └─LayerNorm: 3-44                   1,344\n",
              "│    └─Block: 2-12                            --\n",
              "│    │    └─MultiHeadAttention: 3-45          1,807,008\n",
              "│    │    └─FeedFoward: 3-46                  7,231,392\n",
              "│    │    └─LayerNorm: 3-47                   1,344\n",
              "│    │    └─LayerNorm: 3-48                   1,344\n",
              "├─LayerNorm: 1-4                              1,344\n",
              "├─Linear: 1-5                                 642,042\n",
              "======================================================================\n",
              "Total params: 110,196,858\n",
              "Trainable params: 110,196,858\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#params\n",
        "eval_interval = 2\n",
        "eval_iters = 5\n",
        "n_embd = 672\n",
        "n_head = 16\n",
        "n_layer = 12\n",
        "dropout = 0.0\n",
        "\n",
        "# training\n",
        "epochs = 3000 # no. of epochs to train the model\n",
        "eval_epochs = 100 # no. of epochs to calculate mean test loss\n",
        "info_interval = 250 # test loss information frequency\n",
        "learning_rate = 1e-4\n",
        "\n",
        "\n",
        "\n",
        "# Check if the 'Block' class implementation is available and whether the tokenizer model file 'tokenizer.model' exists.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = NgramLanguageModel(vocab_size, n_embd, block_size, n_layer, n_head, ngram_order, device)\n",
        "model = WxGPT(vocab_size, n_embd, block_size, n_layer, n_head, dropout)\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "summary(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0pCT8rExcL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgsphaJYlEy8"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def avg_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for mode in ['train', 'test']:\n",
        "        losses = torch.zeros(eval_epochs)\n",
        "        for k in range(eval_epochs):\n",
        "            X, Y = random_batch(mode)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[mode] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCLItVgmuox9"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/adam_s/model_v0.01_mts_17.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY_ngLQZlEy-"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, epochs):\n",
        "\n",
        "    results = {\"train_loss\": []}\n",
        "    file_exists = os.path.exists(model_path)\n",
        "\n",
        "    if file_exists == True:\n",
        "        checkpoint = torch.load(model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        if epoch % info_interval == 0 or epoch == epochs - 1:\n",
        "            losses = avg_loss()\n",
        "            print(f\"Step {epoch}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}\")\n",
        "            torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),},'/content/drive/MyDrive/adam_s/model_v0.01_mts_17.pth')\n",
        "            text_input = \"I am\"\n",
        "            generated_text = model.generate(text_input, max_new_tokens=100, temperature=0.8, top_k=None)\n",
        "            print(generated_text)\n",
        "\n",
        "        X, Y = random_batch('train')\n",
        "\n",
        "        logits, loss = model(X, Y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Set an appropriate max_norm value\n",
        "        optimizer.step()\n",
        "        results[\"train_loss\"].append(loss.item())\n",
        "\n",
        "    print(f\"Final train loss: {loss.item()}\")\n",
        "    torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),},'/content/drive/MyDrive/adam_s/model_v0.01_mts_17.pth')\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwbm0DpXXZj"
      },
      "outputs": [],
      "source": [
        "# Set the weight decay value\n",
        "weight_decay = 1e-4  # You can adjust this value based on your specific needs\n",
        "\n",
        "# Create the optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvY_nEJrudfn",
        "outputId": "5cef715c-b2ad-4047-f3f7-aa4104df4ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "file_exists = os.path.exists(model_path)\n",
        "print(file_exists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "bb5eb0febcc44d98b9f6ad28f939c142",
            "67ebf5b080b04d6f912de91ac0b1f3b7",
            "5e5eaab4151948a589b9378ce7bac08c",
            "4e2da4edfbcf4b1f8aaa030f325e3e3a",
            "9eedcc67cd194c7dbebf041f53531835",
            "8879f725a3e044fda1a4cc3d626f8ac4",
            "f4f7a05b08624983950f83359f5fdce3",
            "70dcc1b0f8f346a69b0dd29f2de1255e",
            "c01a3a48cdc9483ab0735a94d38b469d",
            "de6073ed303741978dcd096d18a12373",
            "cc9c68bc213e423692b85dc5cfa8aab8"
          ]
        },
        "id": "gihVNI2CXdvW",
        "outputId": "fae3b9bc-8f95-4069-9456-464dbd512bbf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb5eb0febcc44d98b9f6ad28f939c142",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss 1.2120, test loss 1.3207\n",
            "I am a great best to that altogether with a contusion. If you are mendid. But now, if you cannot be more\n",
            "Step 250: train loss 1.2587, test loss 1.3972\n",
            "I am at my existence of free Branden and the practical countrysiderations of the Africans;125 they raise\n"
          ]
        }
      ],
      "source": [
        "model_results = train(model=model,optimizer=optimizer, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jl2aYqOoXrKI"
      },
      "outputs": [],
      "source": [
        "ax = pd.DataFrame({'Train Loss': [loss for loss in model_results['train_loss']]}).plot(title='Train Loss Decrease', logy=True)\n",
        "\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLM-_WxrlEy_"
      },
      "outputs": [],
      "source": [
        "text_input = \"A man can\"\n",
        "generated_text = model.generate(text_input, max_new_tokens=4500, temperature=1, top_k=0.8)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23035394b9c24fe289d749579d99babb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2e744bb66f4fb3871a04b89eeefc1c",
            "placeholder": "​",
            "style": "IPY_MODEL_41d3b027f42e4a33bccdbd9a5231e591",
            "value": " 19/19 [00:49&lt;00:00,  2.94s/it]"
          }
        },
        "41d3b027f42e4a33bccdbd9a5231e591": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e2da4edfbcf4b1f8aaa030f325e3e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de6073ed303741978dcd096d18a12373",
            "placeholder": "​",
            "style": "IPY_MODEL_cc9c68bc213e423692b85dc5cfa8aab8",
            "value": " 11/3000 [04:34&lt;4:16:05,  5.14s/it]"
          }
        },
        "51aa8149f07a46bf9769d13f132116d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_719b3cdde4a5433da6231f69c86a0059",
              "IPY_MODEL_f5576abaa063471e9550dc050814cb3f",
              "IPY_MODEL_23035394b9c24fe289d749579d99babb"
            ],
            "layout": "IPY_MODEL_cb877dfc72684d148743e4ddaa0e6c29"
          }
        },
        "5e2e744bb66f4fb3871a04b89eeefc1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5eaab4151948a589b9378ce7bac08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70dcc1b0f8f346a69b0dd29f2de1255e",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c01a3a48cdc9483ab0735a94d38b469d",
            "value": 11
          }
        },
        "67ebf5b080b04d6f912de91ac0b1f3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8879f725a3e044fda1a4cc3d626f8ac4",
            "placeholder": "​",
            "style": "IPY_MODEL_f4f7a05b08624983950f83359f5fdce3",
            "value": "  0%"
          }
        },
        "70dcc1b0f8f346a69b0dd29f2de1255e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719b3cdde4a5433da6231f69c86a0059": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed95e740172f4b50ae108727c699b234",
            "placeholder": "​",
            "style": "IPY_MODEL_d053906d458841fe990d193f1d75c2e2",
            "value": "100%"
          }
        },
        "8879f725a3e044fda1a4cc3d626f8ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eedcc67cd194c7dbebf041f53531835": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb5eb0febcc44d98b9f6ad28f939c142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67ebf5b080b04d6f912de91ac0b1f3b7",
              "IPY_MODEL_5e5eaab4151948a589b9378ce7bac08c",
              "IPY_MODEL_4e2da4edfbcf4b1f8aaa030f325e3e3a"
            ],
            "layout": "IPY_MODEL_9eedcc67cd194c7dbebf041f53531835"
          }
        },
        "c01a3a48cdc9483ab0735a94d38b469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7ad2a771fae4323a7348f59fb661279": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb877dfc72684d148743e4ddaa0e6c29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9c68bc213e423692b85dc5cfa8aab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d053906d458841fe990d193f1d75c2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd778e8822db4b6282855caa85fddb31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6073ed303741978dcd096d18a12373": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed95e740172f4b50ae108727c699b234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4f7a05b08624983950f83359f5fdce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5576abaa063471e9550dc050814cb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd778e8822db4b6282855caa85fddb31",
            "max": 19,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7ad2a771fae4323a7348f59fb661279",
            "value": 19
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}